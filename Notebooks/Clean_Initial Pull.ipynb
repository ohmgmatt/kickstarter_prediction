{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import urllib.request\n",
    "from selectolax.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('kickstarter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.loc[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing which script runs faster for the full dataset.\n",
    "There are a few parts to change/improve\n",
    "\n",
    "- How we get the HTML \n",
    "- How we parse the HTML \n",
    "- How we clean the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the HTML\n",
    "At the end of it, we know we want to have only the p tags and if possible h1+ tags and ul tags. We can do this by reading each and appending to a list and joining on that list to create a string for the description. \n",
    "\n",
    "According to https://waymoot.org/home/python_string/, a list comprehension is the most effective method to joining a list of strings. Thus we'll be implementing this into our pull function for all tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring the HTML\n",
    "Before we process and parse HTML, we have to get the HTML first. The two ways I'm focusing on are utilizing the requests library and the urllib library, r and u for function respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Requests & BeautifulSoup\n",
    "def r_bs_pull(url):\n",
    "    try:\n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content)\n",
    "        \n",
    "        textfield = '\\n'.join([item.text for item in soup.find('div', 'full-description').find_all('p')])\n",
    "\n",
    "        return(textfield)\n",
    "    except AttributeError:\n",
    "        return 'Missing Description'\n",
    "    \n",
    "## URLLIB & BeautifulSoup\n",
    "def u_bs_pull(url):\n",
    "    try:\n",
    "        result = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(result.read())\n",
    "    \n",
    "        textfield = '\\n'.join([item.text for item in soup.find('div', 'full-description').find_all('p')])\n",
    "    \n",
    "        return(textfield)\n",
    "    except AttributeError:\n",
    "        return 'Missing Description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 25s, sys: 2.68 s, total: 5min 27s\n",
      "Wall time: 7min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample['r_bs_pull'] = sample['web_url'].apply(r_bs_pull)\n",
    "#7 Min 54s for 201 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 712 ms, total: 29.8 s\n",
      "Wall time: 3min 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample['u_bs_pull'] = sample['web_url'].apply(u_bs_pull)\n",
    "#3min 17s for 201 files\n",
    "#3m 32s for 201 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this experiment, it looks like using urllib in this case is much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML\n",
    "We've optimized the HTML content so now we need to optimize parsing through the HTML. This comparison will be between BeautifulSoup and selectolax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def u_bs_pull(url):\n",
    "#     try:\n",
    "#         result = urllib.request.urlopen(url)\n",
    "#         soup = BeautifulSoup(result.read())\n",
    "    \n",
    "#         textfield = '\\n'.join([item.text for item in soup.find('div', 'full-description').find_all('p')])\n",
    "    \n",
    "#         return(textfield)\n",
    "#     except AttributeError:\n",
    "#         return 'Missing Description'\n",
    "    \n",
    "def u_sl_pull(url):\n",
    "    try:\n",
    "        result = urllib.request.urlopen(url)\n",
    "        parse = HTMLParser(result.read())\n",
    "        \n",
    "        textfield = '\\n'.join([node.text() for node in parse.css_first(\"div.full-description\".css('p'))])\n",
    "        \n",
    "        return(textfield)\n",
    "    except AttributeError:\n",
    "        return 'Missing Description'\n",
    "    \n",
    "## Since we've seen u_bs run, we'll just comapre it with u_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.23 s, sys: 922 ms, total: 5.15 s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample['u_sl_pull'] = sample['web_url'].apply(u_sl_pull)\n",
    "# 2min 59s\n",
    "# 2min 59s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement is very noticable, saving .2 seconds per iteration on average.\n",
    "\n",
    "I want to apply the p tags and also the ul tag so we'll work with BS and see how fast this'll work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_bs_pull_plus(url):\n",
    "    try:\n",
    "        result = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(result.read())\n",
    "    \n",
    "        textfield = '\\n'.join([item.text for item in soup.find('div', 'full-description').find_all(['p','ul'])])\n",
    "    \n",
    "        return(textfield)\n",
    "    except AttributeError:\n",
    "        return 'Missing Description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 677 ms, total: 29.8 s\n",
      "Wall time: 3min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample['u_bs_pull_plus'] = sample['web_url'].apply(u_bs_pull_plus)\n",
    "# 3min 15s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adding the 'ul' tag isn't much different in BS, I'll be using urllibrequest and the beautifulsoup library together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def parallelize(data, func, num_of_processes=8):\n",
    "    data_split = np.array_split(data, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def run_on_subset(func, data_subset):\n",
    "    return data_subset.apply(func, axis=1)\n",
    "\n",
    "def parallelize_on_rows(data, func, num_of_processes=8):\n",
    "    return parallelize(data, partial(run_on_subset, func), num_of_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Missing Description\n",
       "1      Missing Description\n",
       "2      Missing Description\n",
       "3      Missing Description\n",
       "4      Missing Description\n",
       "5      Missing Description\n",
       "6      Missing Description\n",
       "7      Missing Description\n",
       "8      Missing Description\n",
       "9      Missing Description\n",
       "10     Missing Description\n",
       "11     Missing Description\n",
       "12     Missing Description\n",
       "13     Missing Description\n",
       "14     Missing Description\n",
       "15     Missing Description\n",
       "16     Missing Description\n",
       "17     Missing Description\n",
       "18     Missing Description\n",
       "19     Missing Description\n",
       "20     Missing Description\n",
       "21     Missing Description\n",
       "22     Missing Description\n",
       "23     Missing Description\n",
       "24     Missing Description\n",
       "25     Missing Description\n",
       "26     Missing Description\n",
       "27     Missing Description\n",
       "28     Missing Description\n",
       "29     Missing Description\n",
       "              ...         \n",
       "171    Missing Description\n",
       "172    Missing Description\n",
       "173    Missing Description\n",
       "174    Missing Description\n",
       "175    Missing Description\n",
       "176    Missing Description\n",
       "177    Missing Description\n",
       "178    Missing Description\n",
       "179    Missing Description\n",
       "180    Missing Description\n",
       "181    Missing Description\n",
       "182    Missing Description\n",
       "183    Missing Description\n",
       "184    Missing Description\n",
       "185    Missing Description\n",
       "186    Missing Description\n",
       "187    Missing Description\n",
       "188    Missing Description\n",
       "189    Missing Description\n",
       "190    Missing Description\n",
       "191    Missing Description\n",
       "192    Missing Description\n",
       "193    Missing Description\n",
       "194    Missing Description\n",
       "195    Missing Description\n",
       "196    Missing Description\n",
       "197    Missing Description\n",
       "198    Missing Description\n",
       "199    Missing Description\n",
       "200    Missing Description\n",
       "Length: 201, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallelize_on_rows(sample, u_bs_pull_plus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
